{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Presence - Retrieval Augmented Generation Model Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## Overview\n",
    "Retrieval-Augmented Generation (RAG) is an advanced framework that combines retrieval-based and generation-based approaches to enhance the performance of natural language processing (NLP) tasks. It leverages the strengths of both methods to provide more accurate, relevant, and contextually appropriate responses. This hybrid approach is particularly powerful in scenarios where vast amounts of information need to be efficiently accessed and summarized, such as in question answering systems, customer support, and knowledge management.\n",
    "\n",
    "## Components of RAG\n",
    "1. **Document Retrieval:**\n",
    "   - The first step involves retrieving relevant documents or passages from a large corpus of text. This is typically done using vector databases and similarity search techniques. The aim is to narrow down the vast information to a few relevant pieces that can be further processed.\n",
    "\n",
    "2. **Embedding Models:**\n",
    "   - Embedding models transform text data into numerical vectors that capture semantic meanings. Various embedding techniques can be used, such as character-level, word-level, sentence-level, and document-level embeddings. The choice of embedding type depends on the specific use case and the desired level of granularity.\n",
    "\n",
    "3. **Vector Database (e.g., Pinecone):**\n",
    "   - A vector database stores and indexes these embeddings, enabling efficient similarity searches. Pinecone, for instance, is a scalable vector database that supports high-dimensional vector search, making it ideal for real-time retrieval tasks in RAG systems.\n",
    "\n",
    "4. **Language Model (LLM) Prompting:**\n",
    "   - Once the relevant documents are retrieved, a language model (such as GPT-3.5 or similar) generates a response based on the retrieved context and the user's query. This step involves prompt engineering to guide the model in producing high-quality outputs.\n",
    "\n",
    "## How RAG Works\n",
    "1. ‚öôÔ∏è **Query Processing:**\n",
    "   - The user inputs a query. This query is embedded using an embedding model to create a vector representation.\n",
    "   \n",
    "2. üöö **Retrieval Step:** \n",
    "   - The query vector is used to search the vector database, retrieving the most similar documents or passages. This narrows down the information to the most relevant pieces.\n",
    "\n",
    "3. üèóÔ∏è **Generation Step:** \n",
    "   - The retrieved documents are fed into a language model along with the original query. The language model uses this context to generate a coherent and relevant response.\n",
    "\n",
    "4. üé§ **Response Delivery:** \n",
    "   - The generated response is presented to the user, providing a comprehensive answer or summary based on the combined knowledge of the retrieved documents and the language model's generative capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚ùóIMPORTANT INSTRUCTIONS ‚ùó\n",
    "\n",
    "Wherever you see *** in the code, you must replace it with your answer\n",
    "\n",
    "Answers are not directly given to you, just like any developer, you should use the web and find resources if you get a bit stumped. A good place to start looking would be here: \n",
    "\n",
    "[Quickstart](https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/)\n",
    "\n",
    "You can always ask us for help, but we recommend you try to find the answers yourself first! Learning is not a one-shot process!\n",
    "\n",
    "Another way of reaching resources is looking at the documentation of the libraries and functions that we are calling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Import necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Generic Typing\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Imports for a document loading, tokenizing, parsing, etc.\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import PDFMinerParser\n",
    "\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Imports for OpenAI ChatGPT\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import  AzureOpenAIEmbeddings\n",
    "from pydantic.v1 import SecretStr\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_key = os.getenv(\"AZURE_OPENAI_API\")\n",
    "openai_key_secret = SecretStr(openai_key) if openai_key else None \n",
    "azure_deployment = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "azure_emb_deployment = os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "azure_api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### :battery: Load LLM Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatGPT = AzureChatOpenAI(\n",
    "    azure_deployment=azure_deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=azure_api_version or '',\n",
    "    api_key=openai_key_secret,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### :fortune_cookie:Fetch the configuration for the group topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_topic_objects(topic:str) -> dict:\n",
    "\n",
    "    '''    \n",
    "    Parameters:\n",
    "        topic - str: Provides the key topic for which we will load all the necessary variables for the workshop.\n",
    "    \n",
    "    Explanation: \n",
    "    \n",
    "    This function returns a *dictionary* with the keys and value types:\n",
    "        {\n",
    "            'loader': GenericLoader\n",
    "            'text_splitter': TextSplitter,\n",
    "            'prompt_template': String,\n",
    "            'question' : String,\n",
    "        },\n",
    "\n",
    "    This is done to faciliate the configuration of multiple topics for the workshop and facilitate future extensions with little overhead\n",
    "    and little technical skill required to expand.\n",
    "    '''\n",
    "\n",
    "    topic_parameters = {\n",
    "        'health' : \n",
    "            {\n",
    "                'loader': GenericLoader.from_filesystem(\n",
    "                        \"./documents/health\",\n",
    "                        glob=\"*\",\n",
    "                        suffixes=[\".pdf\"],\n",
    "                        parser=PDFMinerParser(),\n",
    "                    ),\n",
    "                'text_splitter': RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000, \n",
    "                        chunk_overlap=200, \n",
    "                        add_start_index=True\n",
    "                    ),\n",
    "                'prompt_template': \"\"\"\n",
    "                    You are a dutch analyst looking working at a research institute company studying the public health sector in the netherlands.\n",
    "                    {context}\n",
    "                    Question: {question}\n",
    "                    Answer the question provided by the human.\n",
    "                    \"\"\",\n",
    "                    'question' : 'Can you give me a non-technical summary of the research findings?'\n",
    "            },\n",
    "        'beverage': \n",
    "            {\n",
    "                'loader': GenericLoader.from_filesystem(\n",
    "                        \"./documents/beverage\",\n",
    "                        glob=\"*\",\n",
    "                        suffixes=[\".pdf\"],\n",
    "                        parser=PDFMinerParser(),\n",
    "                    ),\n",
    "                'text_splitter': RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000, \n",
    "                        chunk_overlap=200, \n",
    "                        add_start_index=True\n",
    "                    ),\n",
    "                'prompt_template': \"\"\"\n",
    "                    You are a senior executive in the food industry analyzing the performane of a beverage company.\n",
    "                    {context}\n",
    "                    Question: {question}\n",
    "                    Answer the question provided by the human.\n",
    "                    \"\"\",\n",
    "                    'question' : 'Can you give me a non-technical summary of the research findings?'\n",
    "            },\n",
    "        'tech': \n",
    "            {\n",
    "                'loader': GenericLoader.from_filesystem(\n",
    "                        \"./documents/tech\",\n",
    "                        glob=\"*\",\n",
    "                        suffixes=[\".pdf\"],\n",
    "                        parser=PDFMinerParser(),\n",
    "                    ),\n",
    "                'text_splitter': RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000, \n",
    "                        chunk_overlap=200, \n",
    "                        add_start_index=True\n",
    "                    ),\n",
    "                'prompt_template': \"\"\"\n",
    "                    You are a senior consultatnt in a tech company analyzing the impact of AI worldwide and its applications.\n",
    "                    {context}\n",
    "                    Question: {question}\n",
    "                    Answer the question provided by the human.\n",
    "                    \"\"\",\n",
    "                    'question' : 'Can you give me a short summary of the findings?'\n",
    "            },\n",
    "        'finance' :\n",
    "            {\n",
    "                'loader': GenericLoader.from_filesystem(\n",
    "                        './documents/finance',\n",
    "                        glob=\"*\",\n",
    "                        suffixes=[\".pdf\"],\n",
    "                        parser=PDFMinerParser(),\n",
    "                    ),\n",
    "                'text_splitter': RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000, \n",
    "                        chunk_overlap=200, \n",
    "                        add_start_index=True\n",
    "                    ),\n",
    "                'prompt_template': \"\"\"\n",
    "                    You are a senior financial analyst analyzing the below document and having a conversation with a human.\n",
    "                    {context}\n",
    "                    Question: {question}\n",
    "                    Answer the question provided by the human.\n",
    "                    \"\"\",\n",
    "                'question' : 'Can you give me a summary of the financial document?'                 \n",
    "            },\n",
    "    }\n",
    "\n",
    "    try: \n",
    "        return topic_parameters[topic]\n",
    "    except KeyError:\n",
    "        message = f'Topic: {topic} is not an valid topic, verify the value or contact a member of the team.'\n",
    "        raise KeyError(message)\n",
    "\n",
    "\n",
    "document_topic = ***\n",
    "pipeline_parameters = fetch_topic_objects(document_topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### :clipboard: Create Document Loaders, Load Documents and Process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document Loaders \n",
    "loader = pipeline_parameters['loader']\n",
    "document_splitter = pipeline_parameters['text_splitter']\n",
    "\n",
    "# Load Documents\n",
    "documents = loader.load()\n",
    "\n",
    "# Process documents chunk them appropriately\n",
    "split_documments = document_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùóHint: Look at the imports\n",
    "embedding_method = ***(\n",
    "    azure_deployment=azure_emb_deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=azure_api_version,\n",
    "    api_key=openai_key_secret,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorDB = Chroma.from_documents(documents=split_documments, embedding=embedding_method)\n",
    "\n",
    "retriever = vectorDB.as_retriever(\n",
    "    search_type= ***\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= ****,\n",
    "    optional_variables=[\"chat_history\"],\n",
    "    template= pipeline_parameters['prompt_template']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üë∑‚Äç‚ôÇÔ∏è Create LangChain Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_langchain_pipeline (\n",
    "        retriever:VectorStoreRetriever,\n",
    "        prompt: PromptTemplate,\n",
    "        llm: BaseChatModel,\n",
    "    ):\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "pipeline = create_rag_langchain_pipeline(\n",
    "        retriever= retriever, \n",
    "        prompt = prompt, \n",
    "        llm = chatGPT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé§ Generate a response using a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = pipeline_parameters[***]\n",
    "\n",
    "print(f'We just asked the following: \\n {question}')\n",
    "\n",
    "result = pipeline.invoke(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have a working RAG (Retrieval-Augmented Generation) Model, we will want our customers/users to be able to retrieve documents freely using natural language.\n",
    "\n",
    "However, it is often the case that these RAG Models perform better when given prompts that provide additional context and guidance in the role they are trying to fulfill.\n",
    "\n",
    "# Understanding Prompt Engineering\n",
    "Prompt engineering is the practice of designing and refining the input prompts given to a language model to achieve the desired output. Essentially, it involves crafting questions or statements in a way that leverages the strengths of the language model, guiding it to produce more accurate, relevant, and contextually appropriate responses. This is crucial for improving the performance of RAG systems, where the quality of retrieved documents and generated responses can directly impact user satisfaction and correctness.\n",
    "\n",
    "We will now be looking at the benefits of prompt engineering and how we can modify the following query to enhance our RAG Model performance.\n",
    "\n",
    "## ‚öôÔ∏è Benefits of Prompt Engineering ‚öôÔ∏è\n",
    "\n",
    "Consinder the query: \"Tell me about climate change.\"\n",
    "\n",
    "it is quite an open-ended and ambigious prompt, leaving the opportunity for the RAG model to hallucinate and generat potentially misleading, useless or incorrect responses. We can _mitigate_ this by employing certain technniques in our prompts.\n",
    "\n",
    "### üí° Contextual Understanding:\n",
    "\n",
    "Prompts that provide specific context can help the language model better understand the user's intent. This reduces ambiguity and ensures that the model retrieves and generates content that is closely aligned with the user's needs.\n",
    "\n",
    "__Modified Query__:\n",
    "\n",
    "\"Tell me about climate change and its impact on coastal cities.\"\n",
    "\n",
    "### üìñ Enhanced Relevance:\n",
    "\n",
    "By framing prompts to include relevant details, users can improve the relevance of the documents retrieved from the vector database. This ensures that the information presented is more pertinent to the query.\n",
    "\n",
    "__Modified Query__:\n",
    "\n",
    "\"Tell me about the effects of climate change on agriculture in North America.\"\n",
    "\n",
    "### üë©‚Äçüè´ Role Specification:\n",
    "\n",
    "Defining the role of the model within the prompt (e.g., \"As an expert in history, summarize the events of World War II\") can help the model generate responses that are more authoritative and tailored to the specified role.\n",
    "\n",
    "__Modified Query__:\n",
    "\n",
    "\"As an environmental scientist, explain the causes and effects of climate change.\"\n",
    "\n",
    "### üèóÔ∏è Guidance and Structure: \n",
    "\n",
    "Structured prompts (e.g., \"Given the following context, provide a summary: [context]\") guide the model on how to approach the response, which can lead to more coherent and well-organized outputs.\n",
    "\n",
    "__Modified Query__:\n",
    "\n",
    "\"Given the following context, provide a summary of the main points about climate change: [context]\"\n",
    "\n",
    "### :abacus: Bias Mitigation:\n",
    "\n",
    "Thoughtfully crafted prompts can help mitigate model biases by steering the model towards neutral and objective language, particularly in sensitive or controversial topics.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Provide an objective overview of climate change, including its causes, effects, and potential solutions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Adding Prompt Engineering to our RAG Model\n",
    "\n",
    "It would be too much to ask from our users/customers to apply all these techniques themselves when they are querying the system for information. Therefore, many applications that employ RAG models do some additional preprocessing to user prompts to leverage the benefits of Prompt Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, your client wants to leverage a RAG Model to behave as a Subject Matter Expert (SME), who is capable of explaining the questions to junior analysts. Their goal is to reduce the amount of time (and therefore money) they spend on training new talent, such that they can quickly get up to speed in their new roles, by leveraging AI, they are looking to expedite knowledge acquisition.\n",
    "\n",
    "Given the topic that has been assigned/selected to you, think of the following properties to add to your query:\n",
    "- üë©‚Äçüè´ Role Specification\n",
    "- üèóÔ∏è Guidance and Structure \n",
    "- üßÆ Bias Mitigation\n",
    "\n",
    "Think carefully of the language you use, the instructions and structure you use, and how you specify the role of your prompt to achieve the correct behavior.\n",
    "\n",
    "Realize that the output of a model is the input to another, how can you ensure that the information is not lost? Or that the model responds appropriately?\n",
    "\n",
    "Here is an example of what not to do:\n",
    "\n",
    "Example:\n",
    "<pre>\n",
    "<strong> User Query </strong>:\n",
    "\n",
    "    Can you tell me who the most famous person in the world is?\n",
    "\n",
    "<strong> Chain Link 1 Prompt </strong>:\n",
    "\n",
    "    You are a celebrity afficionado, up to date to the latest trends and famous people world wide. \n",
    "    Answer the following question:\n",
    "    {query}\n",
    "\n",
    "<strong> Output </strong>:\n",
    "\n",
    "    The most famous man in the world is Dwayne the Rock Jonhson!\n",
    "\n",
    "<strong> Chain Link 2 Prompt - Poor Structure </strong>:\n",
    "\n",
    "    Is this prompt structured and does it contain a lot of information?\n",
    "    {output}\n",
    "\n",
    "<strong> Final Response </strong>:\n",
    "\n",
    "    No, this prompt does not contain a lot of structure, additionally, it only states a single fact.\n",
    "\n",
    "</pre>\n",
    "\n",
    "In this example the _Chain Link 2 Prompt_ was poorly designed, and therefore the output was modified to answer \n",
    "whether the prompt itself was structured - this is not the behaviour we want.\n",
    "\n",
    "Instead a better prompt design would be:\n",
    "\n",
    "<pre>\n",
    "<strong> Chain Link 2 Prompt - Good Structure </strong>\n",
    "\n",
    "    Given the following information: \n",
    "    {output}\n",
    "\n",
    "    Rewrite the response such that:\n",
    "    You indicate the subject, and if known, their profession.\n",
    "    All factual information is in bullet points\n",
    "    You include a short final analysis in which you indicate whether the response is informational or not.\n",
    "\n",
    "</pre>\n",
    "\n",
    "In this example _Chain Link 2 Prompt_ was designed correctly, asking the large language model to modify the contents while restructuring the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided given a role.\n",
    "role_prompt_template = \"\"\" \n",
    "' *** Add your role description here *** ':\n",
    "{llm_response}\n",
    "\"\"\"\n",
    "\n",
    "role_prompt = PromptTemplate(\n",
    "    input_variables=['llm_response'],\n",
    "    template=role_prompt_template,\n",
    ")\n",
    "\n",
    "composed_pipeline_role =  ( \n",
    "    {\"llm_response\": pipeline} \n",
    "    | *** \n",
    "    | *** \n",
    "    | StrOutputParser()\n",
    "    ) \n",
    "\n",
    "result = composed_pipeline_role.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided given a structure.\n",
    "structure_prompt_template = \"\"\" \n",
    "'Add your structure description here':\n",
    "{llm_response}\n",
    "\"\"\"\n",
    "\n",
    "structure_prompt = PromptTemplate(\n",
    "    input_variables=***,\n",
    "    template=***,\n",
    ")\n",
    "\n",
    "composed_pipeline_structure =  ( \n",
    "    {\"llm_response\": pipeline}\n",
    "    | ***\n",
    "    | *** \n",
    "    | StrOutputParser()\n",
    "    ) \n",
    "\n",
    "result = composed_pipeline_structure.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided and verifies whether there are any biases.\n",
    "bias_prompt_template = ***\n",
    "\n",
    "bias_prompt = PromptTemplate(\n",
    "    input_variables=***,\n",
    "    template=***,\n",
    ")\n",
    "\n",
    "composed_pipeline_bias =  ( \n",
    "    {\"llm_response\": pipeline}\n",
    "    | ***\n",
    "    | *** \n",
    "    | ***\n",
    "    ) \n",
    "\n",
    "result = composed_pipeline_bias.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ‚õìÔ∏è Chaining Prompts - Go Beyond a Single Link!\n",
    "\n",
    "As we have seen in the previous exercise, we have been able to chain two pipelines together.\n",
    "\n",
    "However, would we really be proud to call our two chain links a chain? \n",
    "\n",
    "In the following exercise, we will do just this! We will build a chain we can be proud of, where all the previous components are connected to each other - each modifying and performing their task and providing the output to the following chain link!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided given a role.\n",
    "role_prompt_template = ***\n",
    "\n",
    "role_prompt = PromptTemplate(\n",
    "    input_variables=***,\n",
    "    template=***,\n",
    ")\n",
    "\n",
    "composed_pipeline_role =  ( \n",
    "    {\"llm_response\": pipeline} \n",
    "    | *** \n",
    "    | *** \n",
    "    | ***\n",
    "    ) \n",
    "\n",
    "result = composed_pipeline_role.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided given a structure.\n",
    "structure_prompt_template = ***\n",
    "\n",
    "structure_prompt = PromptTemplate(\n",
    "    input_variables=[\"llm_response_sme\"],\n",
    "    template= ***,\n",
    ")\n",
    "\n",
    "composed_pipeline_role_structure =  ( \n",
    "    {\"llm_response_sme\": composed_pipeline_role}\n",
    "    | ***\n",
    "    | *** \n",
    "    | ***\n",
    "    ) \n",
    "\n",
    "result = composed_pipeline_role_structure.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided and verifies whether there are any biases.\n",
    "bias_prompt_template = ***\n",
    "\n",
    "bias_prompt = PromptTemplate(\n",
    "    input_variables=[\"llm_response_sme_struct\"],\n",
    "    template=***,\n",
    ")\n",
    "\n",
    "composed_pipeline_role_structure_bias =  ( \n",
    "    { *** : ***}\n",
    "    | ***\n",
    "    | *** \n",
    "    | ***\n",
    "    ) \n",
    "\n",
    "result = composed_pipeline_role_structure_bias.invoke(question)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UPGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
