{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University Presence - Retrieval Augmented Generation Model Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Intro into Business Case - Company wants to have a RAG to speed up the training of new interns/analysts for a new role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## Overview\n",
    "Retrieval-Augmented Generation (RAG) is an advanced framework that combines retrieval-based and generation-based approaches to enhance the performance of natural language processing (NLP) tasks. It leverages the strengths of both methods to provide more accurate, relevant, and contextually appropriate responses. This hybrid approach is particularly powerful in scenarios where vast amounts of information need to be efficiently accessed and summarized, such as in question answering systems, customer support, and knowledge management.\n",
    "\n",
    "## Components of RAG\n",
    "1. **Document Retrieval:**\n",
    "   - The first step involves retrieving relevant documents or passages from a large corpus of text. This is typically done using vector databases and similarity search techniques. The aim is to narrow down the vast information to a few relevant pieces that can be further processed.\n",
    "\n",
    "2. **Embedding Models:**\n",
    "   - Embedding models transform text data into numerical vectors that capture semantic meanings. Various embedding techniques can be used, such as character-level, word-level, sentence-level, and document-level embeddings. The choice of embedding type depends on the specific use case and the desired level of granularity.\n",
    "\n",
    "3. **Vector Database (e.g., Pinecone):**\n",
    "   - A vector database stores and indexes these embeddings, enabling efficient similarity searches. Pinecone, for instance, is a scalable vector database that supports high-dimensional vector search, making it ideal for real-time retrieval tasks in RAG systems.\n",
    "\n",
    "4. **Language Model (LLM) Prompting:**\n",
    "   - Once the relevant documents are retrieved, a language model (such as GPT-3.5 or similar) generates a response based on the retrieved context and the user's query. This step involves prompt engineering to guide the model in producing high-quality outputs.\n",
    "\n",
    "## How RAG Works\n",
    "1. **Query Processing:**\n",
    "   - The user inputs a query. This query is embedded using an embedding model to create a vector representation.\n",
    "   \n",
    "2. **Retrieval Step:**\n",
    "   - The query vector is used to search the vector database, retrieving the most similar documents or passages. This narrows down the information to the most relevant pieces.\n",
    "\n",
    "3. **Generation Step:**\n",
    "   - The retrieved documents are fed into a language model along with the original query. The language model uses this context to generate a coherent and relevant response.\n",
    "\n",
    "4. **Response Delivery:**\n",
    "   - The generated response is presented to the user, providing a comprehensive answer or summary based on the combined knowledge of the retrieved documents and the language model's generative capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Idea: Create Multiple Datasets  -> Different topics (i.e. Tech documentation, Finance, Law, Ethics and Compliance, Health, etc). Each group gets to pick a dataset to present a business case (How would you sell it to a client?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Generic Typing\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Imports for a website blog for example.\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Imports for a document loading, tokenizing, parsing, etc.\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser, PDFMinerParser\n",
    "\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Imports for OpenAI ChatGPT\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import  OpenAIEmbeddings\n",
    "\n",
    "# Imports for Google Gemini\n",
    "# from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = ''\n",
    "os.environ[\"OPENAPI_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini = ChatVertexAI(model=\"gemini-pro\")\n",
    "chatGPT = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the configuration for the group topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_topic_objects(topic:str) -> dict:\n",
    "\n",
    "    '''    \n",
    "    Parameters:\n",
    "        topic - str: Provides the key topic for which we will load all the necessary variables for the workshop.\n",
    "    \n",
    "    Explanation: \n",
    "    \n",
    "    This function returns a *dictionary* with the keys and value types:\n",
    "        {\n",
    "            'loader': GenericLoader\n",
    "            'text_splitter': TextSplitter,\n",
    "            'prompt_template': String,\n",
    "            'question' : String,\n",
    "        },\n",
    "\n",
    "    This is done to faciliate the configuration of multiple topics for the workshop and facilitate future extensions.\n",
    "    '''\n",
    "\n",
    "    topic_parameters = {\n",
    "        'codebase' : \n",
    "            {\n",
    "                'loader': GenericLoader.from_filesystem(\n",
    "                        \"./documents/code\",\n",
    "                        glob=\"*\",\n",
    "                        suffixes=[\".py\"],\n",
    "                        parser=LanguageParser(),\n",
    "                    ),\n",
    "                'text_splitter': RecursiveCharacterTextSplitter.from_language(\n",
    "                        language= Language.PYTHON,\n",
    "                        chunk_size=100, \n",
    "                        chunk_overlap=0,\n",
    "                    ),\n",
    "                'prompt_template': \"\"\"\n",
    "                    You are a senior Python developer analyzing the below codebase and having a conversation with a human.\n",
    "                    {context}\n",
    "                    {chat_history}\n",
    "                    Question: {question}\n",
    "                    Answer the question provided by the human.\n",
    "                    \"\"\",\n",
    "                'question' : 'Can you tell me what the main components of the code base are?'\n",
    "            },\n",
    "        'scientific': \n",
    "            {\n",
    "                'loader': GenericLoader.from_filesystem(\n",
    "                        \"./documents/science\",\n",
    "                        glob=\"*\",\n",
    "                        suffixes=[\".pdf\"],\n",
    "                        parser=PDFMinerParser(),\n",
    "                    ),\n",
    "                'text_splitter': RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000, \n",
    "                        chunk_overlap=200, \n",
    "                        add_start_index=True\n",
    "                    ),\n",
    "                'prompt_template': \"\"\"\n",
    "                    You are a senior Research and Development Lead in a chemical company analyzing the below research reports and having a conversation with a human.\n",
    "                    {context}\n",
    "                    {chat_history}\n",
    "                    Question: {question}\n",
    "                    Answer the question provided by the human.\n",
    "                    \"\"\",\n",
    "                    'question' : 'Can you give me a non-technical summary of the research findings?'\n",
    "            },\n",
    "        'finance' :\n",
    "            {\n",
    "                'loader': GenericLoader.from_filesystem(\n",
    "                        './documents/finance',\n",
    "                        glob=\"*\",\n",
    "                        suffixes=[\".pdf\"],\n",
    "                        parser=PDFMinerParser(),\n",
    "                    ),\n",
    "                'text_splitter': RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000, \n",
    "                        chunk_overlap=200, \n",
    "                        add_start_index=True\n",
    "                    ),\n",
    "                'prompt_template': \"\"\"\n",
    "                    You are a senior financial analyst analyzing the below document and having a conversation with a human.\n",
    "                    {context}\n",
    "                    {chat_history}\n",
    "                    Question: {question}\n",
    "                    Answer the question provided by the human.\n",
    "                    \"\"\",\n",
    "                'question' : 'Can you give me a summary of the financial document?'                 \n",
    "            },\n",
    "    }\n",
    "\n",
    "    try: \n",
    "        return topic_parameters[topic]\n",
    "    except KeyError:\n",
    "        message = f'Topic: {topic} is not an valid topic, verify the value or contact a member of the team.'\n",
    "        raise KeyError(message)\n",
    "\n",
    "\n",
    "document_topic = \"codebase\"\n",
    "pipeline_parameters = fetch_topic_objects(document_topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document Loaders, Load Documents and Process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document Loaders \n",
    "loader = pipeline_parameters['loader']\n",
    "document_splitter = pipeline_parameters['text_splitter']\n",
    "\n",
    "# Load Documents\n",
    "documents = loader.load()\n",
    "\n",
    "# Process documents chunk them appropriately\n",
    "split_documments = document_splitter.split_documents(documents)\n",
    "\n",
    "embedding_method = OpenAIEmbeddings()\n",
    "\n",
    "vectorDB = Chroma.from_documents(documents=split_documments, embedding=embedding_method)\n",
    "\n",
    "retriever = vectorDB.as_retriever(\n",
    "    search_type='similarity'\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    optional_variables=[\"chat_history\"],\n",
    "    template= pipeline_parameters['prompt_template']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LangChain Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "\n",
    "def create_rag_langchain_pipeline (\n",
    "        retriever:VectorStoreRetriever,\n",
    "        prompt: PromptTemplate,\n",
    "        # prompt_args: Union[List[str], None],\n",
    "        llm: BaseChatModel,\n",
    "    ):\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "pipeline = create_rag_langchain_pipeline(\n",
    "        retriever= retriever, \n",
    "        prompt = prompt, \n",
    "        llm = chatGPT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a response using a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = pipeline_parameters['question']\n",
    "\n",
    "print(f'We just asked the following: \\n {question}')\n",
    "\n",
    "pipeline.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have a working RAG (Retrieval-Augmented Generation) Model, we will want our customers/users to be able to retrieve documents freely using natural language.\n",
    "\n",
    "However, it is often the case that these RAG Models perform better when given prompts that provide additional context and guidance in the role they are trying to fulfill.\n",
    "\n",
    "# Understanding Prompt Engineering\n",
    "Prompt engineering is the practice of designing and refining the input prompts given to a language model to achieve the desired output. Essentially, it involves crafting questions or statements in a way that leverages the strengths of the language model, guiding it to produce more accurate, relevant, and contextually appropriate responses. This is crucial for improving the performance of RAG systems, where the quality of retrieved documents and generated responses can directly impact user satisfaction and correctness.\n",
    "\n",
    "We will now be looking at the benefits of prompt engineering and how we can modify the following query to enhance our RAG Model performance.\n",
    "\n",
    "## Benefits of Prompt Engineering\n",
    "\n",
    "Consinder the query: \"Tell me about climate change.\"\n",
    "\n",
    "it is quite an open-ended and ambigious prompt, leaving the opportunity for the RAG model to hallucinate and generat potentially misleading, useless or incorrect responses. We can _mitigate_ this by employing certain technniques in our prompts.\n",
    "\n",
    "### Contextual Understanding: \n",
    "\n",
    "Prompts that provide specific context can help the language model better understand the user's intent. This reduces ambiguity and ensures that the model retrieves and generates content that is closely aligned with the user's needs.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Tell me about climate change and its impact on coastal cities.\"\n",
    "\n",
    "### Enhanced Relevance: \n",
    "\n",
    "By framing prompts to include relevant details, users can improve the relevance of the documents retrieved from the vector database. This ensures that the information presented is more pertinent to the query.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Tell me about the effects of climate change on agriculture in North America.\"\n",
    "\n",
    "### Role Specification: \n",
    "\n",
    "Defining the role of the model within the prompt (e.g., \"As an expert in history, summarize the events of World War II\") can help the model generate responses that are more authoritative and tailored to the specified role.\n",
    "\n",
    "__Modified Query__:\n",
    "\"As an environmental scientist, explain the causes and effects of climate change.\"\n",
    "\n",
    "###  Guidance and Structure: \n",
    "\n",
    "Structured prompts (e.g., \"Given the following context, provide a summary: [context]\") guide the model on how to approach the response, which can lead to more coherent and well-organized outputs.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Given the following context, provide a summary of the main points about climate change: [context]\"\n",
    "\n",
    "### Bias Mitigation: \n",
    "\n",
    "Thoughtfully crafted prompts can help mitigate model biases by steering the model towards neutral and objective language, particularly in sensitive or controversial topics.\n",
    "\n",
    "__Modified Query__:\n",
    "\"Provide an objective overview of climate change, including its causes, effects, and potential solutions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Prompt Engineering to our RAG Model.\n",
    "\n",
    "It would be too much to ask from our users/customers to apply all these techniques themselves when they are querying the system for information. Therefore, many applications that employ RAG models do some additional preprocessing to user prompts to leverage the benefits of Prompt Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, your client wants to leverage the RAG Model to behave as a Subject Matter Expert (SME), who is capable of explaining the questions to junior Analysts. Their goal is to reduce the amount of time (and therefore money) they spend on training new talent, such that they can quickly get up to speed in their new roles, by leveraging AI, they are looking to expedite knowledge acquisition.\n",
    "\n",
    "Given the topic that has been assigned/selected to you, think of the following properties to add to your query:\n",
    "- Role Specification\n",
    "- Guidance and Structure\n",
    "- Bias Mitigation\n",
    "\n",
    "Think carefully of the language you use, the instructions and structure you use, and how you specify the role of your prompt to achieve the most optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: EDIT THIS --- To do this, we will be using LangChains Composed Chains. This allows us to \"Chain\" together the outputs of our model, asking the model to modify the response given the instruction or structure provided in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA: Use Chains to create a more complex version -> make the students write the composed chain, make it such that it goes something like -> \n",
    "# get prompt -> ask model to answer it in the role provided -> ask the model to structure the response in a particular way, ask the model to read the response and remove the bias\n",
    "#https://python.langchain.com/v0.2/docs/how_to/sequence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided given a role.\n",
    "role_prompt_template = \"\"\" \n",
    "'Add your role description here':\n",
    "{llm_response}\n",
    "\"\"\"\n",
    "\n",
    "role_prompt = PromptTemplate(\n",
    "    input_variables=[\"llm_response\"],\n",
    "    template=role_prompt_template,\n",
    ")\n",
    "\n",
    "composed_pipeline_role =  ( \n",
    "    {\"llm_response\": pipeline} \n",
    "    | role_prompt \n",
    "    | chatGPT \n",
    "    | StrOutputParser()\n",
    "    ) \n",
    "\n",
    "composed_pipeline_role.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided given a structure.\n",
    "structure_prompt_template = \"\"\" \n",
    "'Add your structure description here':\n",
    "{llm_response_sme}\n",
    "\"\"\"\n",
    "\n",
    "structure_prompt = PromptTemplate(\n",
    "    input_variables=[\"llm_response_sme\"],\n",
    "    template=structure_prompt_template,\n",
    ")\n",
    "\n",
    "composed_pipeline_role_structure =  ( \n",
    "    {\"llm_response_sme\": composed_pipeline_role}\n",
    "    | structure_prompt\n",
    "    | chatGPT \n",
    "    | StrOutputParser()\n",
    "    ) \n",
    "\n",
    "composed_pipeline_role_structure.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt which transforms the answer provided and verifies whether there are any biases.\n",
    "bias_prompt_template = \"\"\" \n",
    "'Add your bias prevention description here':\n",
    "{llm_response_sme_struct}\n",
    "\"\"\"\n",
    "\n",
    "bias_prompt = PromptTemplate(\n",
    "    input_variables=[\"llm_response_sme_struct\"],\n",
    "    template=bias_prompt_template,\n",
    ")\n",
    "\n",
    "composed_pipeline_role_structure_bias =  ( \n",
    "    {\"llm_response_sme_struct\": composed_pipeline_role_structure}\n",
    "    | bias_prompt\n",
    "    | chatGPT \n",
    "    | StrOutputParser()\n",
    "    ) \n",
    "\n",
    "composed_pipeline_role_structure_bias.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
